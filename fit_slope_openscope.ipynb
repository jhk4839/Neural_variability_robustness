{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import savemat, loadmat\n",
    "import mat73\n",
    "import hdf5storage as st\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.sparse.linalg import eigsh\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.stats import wilcoxon\n",
    "from scipy.optimize import curve_fit\n",
    "import seaborn as sns\n",
    "from copy import deepcopy as dc\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import cross_val_score, cross_validate, train_test_split, KFold, StratifiedKFold\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "from itertools import combinations\n",
    "import math\n",
    "\n",
    "# # tell pandas to show all columns when we display a DataFrame\n",
    "# pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading variables\n",
    "\n",
    "# openscope\n",
    "with open('SVM_prerequisite_variables.pickle', 'rb') as f:\n",
    "    SVM_prerequisite_variables = pickle.load(f)\n",
    "    \n",
    "    list_rate_w1 = SVM_prerequisite_variables['list_rate_w1'].copy()\n",
    "    list_stm_w1 = SVM_prerequisite_variables['list_stm_w1'].copy()\n",
    "    list_neu_loc = SVM_prerequisite_variables['list_neu_loc'].copy()\n",
    "    list_wfdur = SVM_prerequisite_variables['list_wfdur'].copy()\n",
    "    list_slopes_an_loglog = SVM_prerequisite_variables['list_slopes_an_loglog'].copy()\n",
    "    list_slopes_an_loglog_12 = SVM_prerequisite_variables['list_slopes_an_loglog_12'].copy()\n",
    "\n",
    "num_sess = len(list_rate_w1)\n",
    "\n",
    "# ABO Neuropixels\n",
    "with open('resp_matrix_ep_RS_all_32sess_allensdk.pickle', 'rb') as f:\n",
    "    resp_matrix_ep_RS_all = pickle.load(f)\n",
    "\n",
    "    list_rate_RS = resp_matrix_ep_RS_all['list_rate_RS'].copy()\n",
    "    list_rate_RS_dr = resp_matrix_ep_RS_all['list_rate_RS_dr'].copy()\n",
    "    list_rate_all = resp_matrix_ep_RS_all['list_rate_all'].copy()\n",
    "    list_rate_all_dr = resp_matrix_ep_RS_all['list_rate_all_dr'].copy()\n",
    "    list_slopes_RS_an_loglog = resp_matrix_ep_RS_all['list_slopes_RS_an_loglog'].copy()\n",
    "    list_slopes_all_an_loglog = resp_matrix_ep_RS_all['list_slopes_all_an_loglog'].copy()\n",
    "\n",
    "    sess_inds_qual_all = resp_matrix_ep_RS_all['sess_inds_qual_all'].copy()\n",
    "    sess_inds_qual_all_dr = resp_matrix_ep_RS_all['sess_inds_qual_all_dr'].copy()\n",
    "\n",
    "num_sess_ABO = len(list_rate_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mean_var_trial(label_cnt_dict, rate_sorted):    \n",
    "    list_trial_mean = [[0]] * len(label_cnt_dict)\n",
    "    list_trial_var = [[0]] * len(label_cnt_dict)\n",
    "\n",
    "    for trial_ind, trial_type in enumerate(label_cnt_dict):\n",
    "        \n",
    "        trial_rate = np.array(rate_sorted.loc[:, trial_type])                \n",
    "        trial_mean = np.mean(trial_rate, axis=1)\n",
    "        trial_var = np.var(trial_rate, axis=1, ddof=1)\n",
    "\n",
    "        trial_mean = pd.DataFrame(trial_mean, columns=[trial_type], index=rate_sorted.index)\n",
    "        trial_var = pd.DataFrame(trial_var, columns=[trial_type], index=rate_sorted.index)\n",
    "        list_trial_mean[trial_ind] = pd.concat([trial_mean] * label_cnt_dict[trial_type], axis=1)\n",
    "        list_trial_var[trial_ind] = pd.concat([trial_var] * label_cnt_dict[trial_type], axis=1)\n",
    "\n",
    "    rate_sorted_mean = pd.concat(list_trial_mean, axis=1)\n",
    "    rate_sorted_var = pd.concat(list_trial_var, axis=1)\n",
    "\n",
    "    return rate_sorted_mean, rate_sorted_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mean_var_trial_collapse(label_cnt_dict, rate_sorted):    \n",
    "    list_trial_mean = [[0]] * len(label_cnt_dict)\n",
    "    list_trial_var = [[0]] * len(label_cnt_dict)\n",
    "\n",
    "    for trial_ind, trial_type in enumerate(label_cnt_dict):\n",
    "        \n",
    "        trial_rate = np.array(rate_sorted.loc[:, trial_type])                \n",
    "        trial_mean = np.mean(trial_rate, axis=1, dtype=np.longdouble)\n",
    "        trial_var = np.var(trial_rate, axis=1, ddof=1, dtype=np.longdouble)\n",
    "\n",
    "        trial_mean = pd.DataFrame(trial_mean, columns=[trial_type], index=rate_sorted.index)\n",
    "        trial_var = pd.DataFrame(trial_var, columns=[trial_type], index=rate_sorted.index)\n",
    "        list_trial_mean[trial_ind] = trial_mean.copy()\n",
    "        list_trial_var[trial_ind] = trial_var.copy()\n",
    "\n",
    "    rate_sorted_mean = pd.concat(list_trial_mean, axis=1)\n",
    "    rate_sorted_var = pd.concat(list_trial_var, axis=1)\n",
    "\n",
    "    return rate_sorted_mean, rate_sorted_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # data loading (Neuropixels)\n",
    "\n",
    "# list_rate_w1 = []\n",
    "# list_stm_w1 = []\n",
    "\n",
    "# list_rate_w0 = []\n",
    "# list_stm_w0 = []\n",
    "\n",
    "# list_rate_k1 = []\n",
    "# list_stm_k1 = []\n",
    "\n",
    "# list_rate_k0 = []\n",
    "# list_stm_k0 = []\n",
    "\n",
    "# list_neu_loc = []\n",
    "\n",
    "# list_wfdur = []\n",
    "\n",
    "# list_ep_folders = ['sub_619296', 'sub_620333', 'sub_620334', \\\n",
    "#     'sub_625545', 'sub_625554', 'sub_625555', 'sub_630506', 'sub_631510', \\\n",
    "#     'sub_631570', 'sub_633229', 'sub_637484']\n",
    "# path_ep_folders = 'D:\\\\Users\\\\USER\\\\MATLAB\\\\OpenScopeData_00248_v240130_postprocessed\\\\'\n",
    "# file_name_pp = '\\\\postprocessed.mat'\n",
    "# file_name_qc = '\\\\qc_units.mat'\n",
    "\n",
    "# for ep_folder in list_ep_folders:\n",
    "#     pp = mat73.loadmat(path_ep_folders + ep_folder + file_name_pp)\n",
    "#     qc = loadmat(path_ep_folders + ep_folder + file_name_qc)\n",
    "\n",
    "#     # data loading\n",
    "#     rate_w1 = np.squeeze(pp['Rall']['ICwcfg1_presentations'])\n",
    "#     stm_w1 = np.squeeze(pp['vis']['ICwcfg1_presentations']['trialorder'])\n",
    "\n",
    "#     rate_w0 = np.squeeze(pp['Rall']['ICwcfg0_presentations'])\n",
    "#     stm_w0 = np.squeeze(pp['vis']['ICwcfg0_presentations']['trialorder'])\n",
    "\n",
    "#     rate_k1 = np.squeeze(pp['Rall']['ICkcfg1_presentations'])\n",
    "#     stm_k1 = np.squeeze(pp['vis']['ICkcfg1_presentations']['trialorder'])\n",
    "\n",
    "#     rate_k0 = np.squeeze(pp['Rall']['ICkcfg0_presentations'])\n",
    "#     stm_k0 = np.squeeze(pp['vis']['ICkcfg0_presentations']['trialorder'])\n",
    "\n",
    "#     neualloc = np.squeeze(pp['neuallloc'])\n",
    "\n",
    "#     wfdur = np.squeeze(qc['unit_wfdur'])\n",
    "\n",
    "#     list_rate_w1.append(rate_w1)\n",
    "#     list_stm_w1.append(stm_w1)\n",
    "\n",
    "#     list_rate_w0.append(rate_w0)\n",
    "#     list_stm_w0.append(stm_w0)\n",
    "\n",
    "#     list_rate_k1.append(rate_k1)\n",
    "#     list_stm_k1.append(stm_k1)\n",
    "\n",
    "#     list_rate_k0.append(rate_k0)\n",
    "#     list_stm_k0.append(stm_k0)\n",
    "\n",
    "#     list_neu_loc.append(neualloc)\n",
    "\n",
    "#     list_wfdur.append(wfdur)\n",
    "\n",
    "#     print(ep_folder)\n",
    "\n",
    "# num_sess = len(list_rate_w1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Linear Fitting Slope across neurons (Ic Neuropixels) (loglog line)\n",
    "\n",
    "# fig, axes = plt.subplots(1, 1, figsize=(5, 4))\n",
    "# axes = np.array(axes).flatten()\n",
    "\n",
    "# # Iterate over all sessions\n",
    "# list_slopes_an_loglog = np.empty(num_sess, dtype=object)\n",
    "# for sess_ind, (rate_all, stm, neu_loc, wfdur) in enumerate(zip(list_rate_w1, list_stm_w1, list_neu_loc, list_wfdur)):\n",
    "#     print(f'session index: {sess_ind}')\n",
    "\n",
    "#     # rate_all transposition\n",
    "#     rate_all = rate_all.T.copy()\n",
    "\n",
    "#     ser_neu_loc = pd.Series(neu_loc)\n",
    "\n",
    "#     # Extract V1 neurons\n",
    "#     list_vis = [ser_neu_loc.str.contains('VISp'), ~ser_neu_loc.str.contains('VISpm')]\n",
    "#     list_vis = [all(bools) for bools in zip(*list_vis)]\n",
    "#     rate = rate_all[list_vis].copy()\n",
    "#     # list_visp_rs = [ser_neu_loc.str.contains('VISp'), ~ser_neu_loc.str.contains('VISpm'), (wfdur >= 0.4)]\n",
    "#     # rate = rate_all[[all(bools) for bools in zip(*list_visp_rs)]].copy()\n",
    "#     # print(np.sum([all(bools) for bools in zip(*list_visp_rs)]))\n",
    "\n",
    "#     # Multiply by delta t to convert to spike counts\n",
    "#     rate = rate * 0.4\n",
    "\n",
    "#     # Create a counting dictionary for each stimulus\n",
    "#     all_stm_unique, all_stm_counts = np.unique(stm, return_counts=True) \n",
    "#     stm_cnt_dict = dict(zip(all_stm_unique, all_stm_counts))\n",
    "#     dict_trial_type = {0: 'Blank', 1: 'X', 2:'Tc1', 3: 'Ic1', 4: 'Lc1', 5:'Tc2', 6: 'Lc2', 7: 'Ic2', \\\n",
    "#         8: 'Ire1', 9: 'Ire2', 10: 'Tre1', 11: 'Tre2', 12: 'Xre1', 13: 'Xre2', 14: 'BR_in', 15: 'BL_in', \\\n",
    "#             16: 'TL_in', 17: 'TR_in', 18: 'BR_out', 19: 'BL_out', 20: 'TL_out', 21: 'TR_out'}\n",
    "    \n",
    "#     # Create label array for all stimuli\n",
    "#     label = []\n",
    "#     for i in stm:\n",
    "#         for trial_type_num in dict_trial_type:\n",
    "#             if i == trial_type_num:\n",
    "#                 label.append(dict_trial_type[trial_type_num])\n",
    "#     label = np.array(label)\n",
    "\n",
    "#     # convert to dataframe\n",
    "#     rate = pd.DataFrame(rate, columns=label)\n",
    "\n",
    "#     # sort trials based on stimuli\n",
    "#     rate_sorted = rate.sort_index(axis=1)\n",
    "#     label_sorted = rate_sorted.columns.copy()\n",
    "\n",
    "#     # Create a counting dictionary for each stimulus \n",
    "#     all_label_unique, all_label_counts = np.unique(label_sorted, return_counts=True) \n",
    "#     label_cnt_dict = dict(zip(all_label_unique, all_label_counts))\n",
    "\n",
    "#     # Compute mean & variance for each stimulus\n",
    "#     rate_sorted_mean, rate_sorted_var = compute_mean_var_trial(label_cnt_dict, rate_sorted)\n",
    "#     rate_sorted_mean_coll, rate_sorted_var_coll = compute_mean_var_trial_collapse(label_cnt_dict, rate_sorted)\n",
    "\n",
    "#     # Calculate and collect linear slopes for all stimuli\n",
    "#     slopes = np.zeros((2, rate_sorted_mean_coll.shape[1]))\n",
    "#     for trial_type_ind, trial_type in enumerate(rate_sorted_mean_coll.columns):\n",
    "            \n",
    "#         bool_mean_notzero = rate_sorted_mean_coll.loc[:, trial_type] > 0\n",
    "#         popt = np.polyfit(np.log10(rate_sorted_mean.loc[bool_mean_notzero, trial_type].values).flatten().astype(np.float32), \\\n",
    "#                             np.log10(rate_sorted_var.loc[bool_mean_notzero, trial_type].values).flatten().astype(np.float32), 1)\n",
    "#         # popt = np.polyfit(np.log10(rate_sorted_mean.loc[:, trial_type].values).flatten().astype(np.float32), \\\n",
    "#         #                     np.log10(rate_sorted_var.loc[:, trial_type].values).flatten().astype(np.float32), 1)\n",
    "        \n",
    "#         slopes[:, trial_type_ind] = popt.copy()\n",
    "            \n",
    "#     list_slopes_an_loglog[sess_ind] = slopes.copy()\n",
    "\n",
    "# # Histogram of slopes (all sessions)\n",
    "# # fig, ax = plt.subplots(figsize=(5, 4))\n",
    "\n",
    "# list_slopes_an_loglog_flattened = np.concatenate([slopes[0, :].flatten() for slopes in list_slopes_an_loglog if slopes is not None])\n",
    "# # list_slopes_an_loglog_flattened = np.concatenate((list_slopes_an_loglog_flattened[:1537], list_slopes_an_loglog_flattened[1538:]))\n",
    "\n",
    "# bin_size = 0.01\n",
    "# lower_bound, upper_bound = math.floor(np.min(list_slopes_an_loglog_flattened)), \\\n",
    "#     math.ceil(np.max(list_slopes_an_loglog_flattened))\n",
    "\n",
    "# weights=np.ones_like(list_slopes_an_loglog_flattened)/len(list_slopes_an_loglog_flattened)\n",
    "# axes[0].hist(list_slopes_an_loglog_flattened, bins=np.arange(lower_bound, upper_bound, bin_size), range=(lower_bound, upper_bound), \\\n",
    "#         weights=weights, color='cornflowerblue')\n",
    "# # plt.axvline(1, color='r', linestyle='--')\n",
    "\n",
    "# axes[0].set_xlabel('log(mean)-log(var) slope', fontsize=20)\n",
    "# # if area_ind == 0 or area_ind == 3:\n",
    "# axes[0].set_ylabel('Frequency of stimuli', fontsize=20)\n",
    "# axes[0].set_xticks([0, 0.5, 1, 1.5, 2])\n",
    "# # axes[0].set_yticks(np.arange(5, 40, 5))\n",
    "# axes[0].tick_params('both', labelsize=18)\n",
    "\n",
    "# # Wilcoxon signed-rank test\n",
    "# wilc = list(wilcoxon(list_slopes_an_loglog_flattened, np.ones_like(list_slopes_an_loglog_flattened), alternative='greater'))\n",
    "# # plt.annotate(f'p = {wilc[1]:.3f}', xy=(0.8, 0.9), xycoords='axes fraction')\n",
    "\n",
    "# print(f'slope median = {np.median(list_slopes_an_loglog_flattened):.2f}')\n",
    "\n",
    "# plt.subplots_adjust(hspace=0.4, wspace=0.3)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Linear Fitting Slope across neurons (Ic Neuropixels) (loglog line) (high-repeat trial types)\n",
    "\n",
    "# fig, axes = plt.subplots(1, 1, figsize=(5, 4))\n",
    "# axes = np.array(axes).flatten()\n",
    "\n",
    "# # Iterate over all sessions\n",
    "# list_slopes_an_loglog_12 = np.empty(num_sess, dtype=object)\n",
    "# for sess_ind, (rate_all, stm, neu_loc, wfdur) in enumerate(zip(list_rate_w1, list_stm_w1, list_neu_loc, list_wfdur)):\n",
    "#     print(f'session index: {sess_ind}')\n",
    "\n",
    "#     # rate_all transposition\n",
    "#     rate_all = rate_all.T.copy()\n",
    "\n",
    "#     ser_neu_loc = pd.Series(neu_loc)\n",
    "\n",
    "#     # Extract V1 neurons\n",
    "#     list_vis = [ser_neu_loc.str.contains('VISp'), ~ser_neu_loc.str.contains('VISpm')]\n",
    "#     list_vis = [all(bools) for bools in zip(*list_vis)]\n",
    "#     rate = rate_all[list_vis].copy()\n",
    "#     # list_visp_rs = [ser_neu_loc.str.contains('VISp'), ~ser_neu_loc.str.contains('VISpm'), (wfdur >= 0.4)]\n",
    "#     # rate = rate_all[[all(bools) for bools in zip(*list_visp_rs)]].copy()\n",
    "#     # print(np.sum([all(bools) for bools in zip(*list_visp_rs)]))\n",
    "\n",
    "#     # Multiply by delta t to convert to spike counts\n",
    "#     rate = rate * 0.4\n",
    "\n",
    "#     # Create a counting dictionary for each stimulus\n",
    "#     all_stm_unique, all_stm_counts = np.unique(stm, return_counts=True) \n",
    "#     stm_cnt_dict = dict(zip(all_stm_unique, all_stm_counts))\n",
    "#     dict_trial_type = {0: 'Blank', 1: 'X', 2:'Tc1', 3: 'Ic1', 4: 'Lc1', 5:'Tc2', 6: 'Lc2', 7: 'Ic2', \\\n",
    "#         8: 'Ire1', 9: 'Ire2', 10: 'Tre1', 11: 'Tre2', 12: 'Xre1', 13: 'Xre2', 14: 'BR_in', 15: 'BL_in', \\\n",
    "#             16: 'TL_in', 17: 'TR_in', 18: 'BR_out', 19: 'BL_out', 20: 'TL_out', 21: 'TR_out'}\n",
    "    \n",
    "#     # Create label array for all stimuli\n",
    "#     label = []\n",
    "#     for i in stm:\n",
    "#         for trial_type_num in dict_trial_type:\n",
    "#             if i == trial_type_num:\n",
    "#                 label.append(dict_trial_type[trial_type_num])\n",
    "#     label = np.array(label)\n",
    "\n",
    "#     # convert to dataframe\n",
    "#     rate = pd.DataFrame(rate, columns=label)\n",
    "\n",
    "#     # sort trials based on stimuli\n",
    "#     rate_sorted = rate.sort_index(axis=1)\n",
    "#     label_sorted = rate_sorted.columns.copy()\n",
    "\n",
    "#     # Create a counting dictionary for each stimulus \n",
    "#     all_label_unique, all_label_counts = np.unique(label_sorted, return_counts=True) \n",
    "#     label_cnt_dict = dict(zip(all_label_unique, all_label_counts))\n",
    "\n",
    "#     # Extract stimuli having 400 repeats (trials)\n",
    "#     rate_sorted = rate_sorted.loc[:, all_label_unique[all_label_counts == 400]].copy()\n",
    "#     label_cnt_dict = dict(zip(all_label_unique[all_label_counts == 400], \\\n",
    "#                                     np.full((len(all_label_unique[all_label_counts == 400])), 400)))\n",
    "\n",
    "#     # Compute mean & variance for each stimulus\n",
    "#     rate_sorted_mean, rate_sorted_var = compute_mean_var_trial(label_cnt_dict, rate_sorted)\n",
    "#     rate_sorted_mean_coll, rate_sorted_var_coll = compute_mean_var_trial_collapse(label_cnt_dict, rate_sorted)\n",
    "\n",
    "#     # Calculate and collect linear slopes for all stimuli\n",
    "#     slopes = np.zeros((2, rate_sorted_mean_coll.shape[1]))\n",
    "#     for trial_type_ind, trial_type in enumerate(rate_sorted_mean_coll.columns):\n",
    "            \n",
    "#         bool_mean_notzero = rate_sorted_mean_coll.loc[:, trial_type] > 0\n",
    "#         popt = np.polyfit(np.log10(rate_sorted_mean.loc[bool_mean_notzero, trial_type].values).flatten().astype(np.float32), \\\n",
    "#                             np.log10(rate_sorted_var.loc[bool_mean_notzero, trial_type].values).flatten().astype(np.float32), 1)\n",
    "#         # popt = np.polyfit(np.log10(rate_sorted_mean.loc[:, trial_type].values).flatten().astype(np.float32), \\\n",
    "#         #                     np.log10(rate_sorted_var.loc[:, trial_type].values).flatten().astype(np.float32), 1)\n",
    "        \n",
    "#         slopes[:, trial_type_ind] = popt.copy()\n",
    "            \n",
    "#     list_slopes_an_loglog_12[sess_ind] = slopes.copy()\n",
    "\n",
    "# # Histogram of slopes (all sessions)\n",
    "# # fig, ax = plt.subplots(figsize=(5, 4))\n",
    "\n",
    "# list_slopes_an_loglog_flattened = np.concatenate([slopes[0, :].flatten() for slopes in list_slopes_an_loglog_12 if slopes is not None])\n",
    "# # list_slopes_an_loglog_flattened = np.concatenate((list_slopes_an_loglog_flattened[:1537], list_slopes_an_loglog_flattened[1538:]))\n",
    "\n",
    "# bin_size = 0.01\n",
    "# lower_bound, upper_bound = math.floor(np.min(list_slopes_an_loglog_flattened)), \\\n",
    "#     math.ceil(np.max(list_slopes_an_loglog_flattened))\n",
    "\n",
    "# weights=np.ones_like(list_slopes_an_loglog_flattened)/len(list_slopes_an_loglog_flattened)\n",
    "# axes[0].hist(list_slopes_an_loglog_flattened, bins=np.arange(lower_bound, upper_bound, bin_size), range=(lower_bound, upper_bound), \\\n",
    "#         weights=weights, color='cornflowerblue')\n",
    "# # plt.axvline(1, color='r', linestyle='--')\n",
    "\n",
    "# axes[0].set_xlabel('log(mean)-log(var) slope', fontsize=20)\n",
    "# # if area_ind == 0 or area_ind == 3:\n",
    "# axes[0].set_ylabel('Frequency of stimuli', fontsize=20)\n",
    "# axes[0].set_xticks([0, 0.5, 1, 1.5, 2])\n",
    "# # axes[0].set_yticks(np.arange(5, 40, 5))\n",
    "# axes[0].tick_params('both', labelsize=18)\n",
    "\n",
    "# # Wilcoxon signed-rank test\n",
    "# wilc = list(wilcoxon(list_slopes_an_loglog_flattened, np.ones_like(list_slopes_an_loglog_flattened), alternative='greater'))\n",
    "# # plt.annotate(f'p = {wilc[1]:.3f}', xy=(0.8, 0.9), xycoords='axes fraction')\n",
    "\n",
    "# print(f'slope median = {np.median(list_slopes_an_loglog_flattened):.2f}')\n",
    "\n",
    "# plt.subplots_adjust(hspace=0.4, wspace=0.3)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('SVM_prerequisite_variables.pickle', 'wb') as f:\n",
    "#     pickle.dump({'tree_variables': ['list_rate_w1', 'list_stm_w1', 'list_neu_loc', 'list_wfdur',\n",
    "#                                     'list_slopes_an_loglog', 'list_slopes_an_loglog_12'],\n",
    "#                                     'list_rate_w1': list_rate_w1, 'list_stm_w1': list_stm_w1,\n",
    "#                                     'list_neu_loc': list_neu_loc, 'list_wfdur': list_wfdur,\n",
    "#                                     'list_slopes_an_loglog': list_slopes_an_loglog, 'list_slopes_an_loglog_12': list_slopes_an_loglog_12}, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "allensdk_cl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
